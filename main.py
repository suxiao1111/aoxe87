import asyncio
import json
import time
import uuid
import httpx
import uvicorn
import sys
import os
from fastapi import FastAPI, Request, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from typing import Dict, Any, Optional, List, Generator

# --- Configuration ---
PORT = int(os.environ.get("PORT", 7860))
API_KEY = os.environ.get("API_KEY", None)  # Optional API Key for security
HEADLESS = os.environ.get("HEADLESS", "false").lower() == "true"

MODELS_CONFIG_FILE = "models.json"
STATS_FILE = "stats.json"

# --- Token Stats Manager ---
class TokenStatsManager:
    def __init__(self, filepath=STATS_FILE):
        self.filepath = filepath
        self.stats = {"total_requests": 0, "total_tokens": 0, "prompt_tokens": 0, "completion_tokens": 0}
        self.lock = asyncio.Lock()
        self.load_stats()

    def load_stats(self):
        try:
            with open(self.filepath, 'r', encoding='utf-8') as f:
                self.stats = json.load(f)
        except FileNotFoundError:
            self.save_stats()
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading stats: {e}")

    def save_stats(self):
        try:
            with open(self.filepath, 'w', encoding='utf-8') as f:
                json.dump(self.stats, f, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è Error saving stats: {e}")

    async def update(self, prompt_tokens, completion_tokens):
        async with self.lock:
            self.stats["total_requests"] += 1
            self.stats["prompt_tokens"] += prompt_tokens
            self.stats["completion_tokens"] += completion_tokens
            self.stats["total_tokens"] += (prompt_tokens + completion_tokens)
            self.save_stats()

stats_manager = TokenStatsManager()

# --- Credential Manager ---
class CredentialManager:
    def __init__(self, filepath="credentials.json"):
        self.filepath = filepath
        self.latest_harvest: Optional[Dict[str, Any]] = None
        self.last_updated: float = 0
        self._refresh_event = None
        self._refresh_complete_event = None
        self._refresh_lock = None
        self.load_from_disk()

    @property
    def refresh_event(self):
        if self._refresh_event is None:
            self._refresh_event = asyncio.Event()
            self._refresh_event.set()
        return self._refresh_event

    @property
    def refresh_complete_event(self):
        if self._refresh_complete_event is None:
            self._refresh_complete_event = asyncio.Event()
            self._refresh_complete_event.set()
        return self._refresh_complete_event

    @property
    def refresh_lock(self):
        if self._refresh_lock is None:
            self._refresh_lock = asyncio.Lock()
        return self._refresh_lock

    def load_from_disk(self):
        try:
            with open(self.filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.latest_harvest = data.get('harvest')
                self.last_updated = data.get('timestamp', 0)
                print(f"üìÇ Loaded credentials from disk (Age: {int(time.time() - self.last_updated)}s)")
        except FileNotFoundError:
            print("üìÇ No saved credentials found.")
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading credentials: {e}")

    def save_to_disk(self):
        try:
            with open(self.filepath, 'w', encoding='utf-8') as f:
                json.dump({
                    'harvest': self.latest_harvest,
                    'timestamp': self.last_updated
                }, f, indent=2)
            print(f"üíæ Credentials saved to {self.filepath}")
        except Exception as e:
            print(f"‚ö†Ô∏è Error saving credentials: {e}")

    def update(self, data: Dict[str, Any]):
        self.latest_harvest = data
        self.last_updated = time.time()
        print(f"üîÑ Credentials updated at {time.strftime('%H:%M:%S')}")
        self.save_to_disk()
        self.refresh_event.set() # Unblock credential waiting requests

    def update_token(self, token: str):
        if self.latest_harvest and 'headers' in self.latest_harvest:
            # Debug: Print old token prefix
            old_val = self.latest_harvest['headers'].get('X-Goog-First-Party-Reauth', 'None')
            print(f"üîç Old Token Prefix: {old_val[:20]}...")

            # Update the specific header.
            formatted_token = json.dumps([token])
            self.latest_harvest['headers']['X-Goog-First-Party-Reauth'] = formatted_token
            
            print(f"üîç New Token Prefix: {formatted_token[:20]}...")
            
            self.last_updated = time.time()
            print(f"üîÑ Token refreshed via WebSocket at {time.strftime('%H:%M:%S')}")
            self.save_to_disk()
            self.refresh_event.set() # Unblock waiting requests

    async def wait_for_refresh(self, timeout=30):
        """Blocks until new credentials are received or timeout occurs."""
        self.refresh_event.clear() # Start blocking for credentials
        self.refresh_complete_event.clear() # Also block for UI completion signal
        try:
            print("   - Waiting for credentials...")
            await asyncio.wait_for(self.refresh_event.wait(), timeout=timeout)
            return True
        except asyncio.TimeoutError:
            print("   - Timed out waiting for credentials.")
            self.refresh_complete_event.set() # Unblock the other wait if this one fails
            return False

    async def wait_for_refresh_complete(self, timeout=30):
        """Blocks until the frontend signals the refresh sequence is fully complete."""
        try:
            print("   - Waiting for frontend UI to be ready...")
            await asyncio.wait_for(self.refresh_complete_event.wait(), timeout=timeout)
            return True
        except asyncio.TimeoutError:
            print("   - Timed out waiting for frontend UI.")
            return False

    def get_credentials(self) -> Optional[Dict[str, Any]]:
        if not self.latest_harvest:
            return None
        # Simple freshness check (warn if older than 10 minutes)
        # Note: Vertex AI tokens are short-lived, but cookies might last longer.
        # We'll just warn for now.
        if time.time() - self.last_updated > 1800: # 30 mins
            print("‚ö†Ô∏è Warning: Credentials might be stale (>30 mins old).")
        return self.latest_harvest

cred_manager = CredentialManager()

# --- Vertex AI Client ---
class AuthError(Exception):
    """Raised when authentication fails (e.g. Recaptcha invalid)."""
    pass

class VertexAIClient:
    def __init__(self):
        # Increase connection limits for concurrency
        limits = httpx.Limits(max_keepalive_connections=20, max_connections=100)
        self.client = httpx.AsyncClient(timeout=120.0, limits=limits)

    async def complete_chat(self, messages: List[Dict[str, str]], model: str, **kwargs) -> Dict[str, Any]:
        """Aggregates the streaming response into a single non-streaming ChatCompletion object."""
        
        full_content = ""
        reasoning_content = ""
        finish_reason = "stop"
        
        # Use the existing streaming logic to get chunks
        async for chunk_data_sse in self.stream_chat(messages, model, **kwargs):
            # SSE format: "data: {json_chunk}\n\n"
            if chunk_data_sse.startswith("data: "):
                json_str = chunk_data_sse[6:].strip()
                if json_str == "[DONE]":
                    continue
                
                try:
                    chunk = json.loads(json_str)
                    choices = chunk.get('choices', [])
                    if choices:
                        delta = choices[0].get('delta', {})
                        
                        # Aggregate content
                        if 'content' in delta:
                            full_content += delta['content']
                        if 'reasoning_content' in delta:
                            reasoning_content += delta['reasoning_content']
                            
                        # Capture finish reason from the last chunk
                        if choices[0].get('finish_reason'):
                            finish_reason = choices[0]['finish_reason']
                            
                except json.JSONDecodeError as e:
                    print(f"Error decoding JSON chunk in complete_chat: {e}")
                    # Continue to next chunk
                    
        # Construct the final non-streaming response
        # Note: We are not calculating token usage here, as that requires more complex logic
        # and is usually done by the upstream API. We will use placeholders.
        
        # Combine reasoning and content if reasoning exists
        final_content = full_content
        if reasoning_content:
            final_content = f"**Reasoning:**\n{reasoning_content}\n\n**Response:**\n{full_content}"
        
        # Workaround for clients that treat empty content as failure
        if not final_content:
            final_content = " "
            
        response = {
            "id": f"chatcmpl-proxy-nonstream-{uuid.uuid4()}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "usage": {
                "prompt_tokens": 0, # Placeholder
                "completion_tokens": 0, # Placeholder
                "total_tokens": 0 # Placeholder
            },
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": final_content
                    },
                    "finish_reason": finish_reason
                }
            ]
        }
        return response

    async def stream_chat(self, messages: List[Dict[str, str]], model: str, **kwargs):
        # 1. Check Credential Freshness & Auto-Refresh
        # Vertex AI tokens typically last 1 hour. We'll refresh if older than 50 mins.
        
        # Use a lock to prevent multiple requests from triggering refresh simultaneously
        if not cred_manager.latest_harvest or (time.time() - cred_manager.last_updated > 3000):
            async with cred_manager.refresh_lock:
                # Double check inside lock
                should_refresh = False
                if not cred_manager.latest_harvest:
                    should_refresh = True
                elif time.time() - cred_manager.last_updated > 3000:
                    print("‚ö†Ô∏è Credentials are stale (>50 mins). Triggering pre-flight refresh...")
                    should_refresh = True
                
                if should_refresh:
                    # Trigger refresh
                    await request_token_refresh()
                    
                    # Wait for credentials (with a timeout)
                    print("‚è≥ Waiting for fresh credentials...")
                    refreshed = await cred_manager.wait_for_refresh(timeout=60)
                    
                    if refreshed:
                        # Add 1 second delay after token is received and refresh_event is set
                        await asyncio.sleep(1)
                    
                    if not refreshed and not cred_manager.latest_harvest:
                        # Only fail if we have NO credentials at all.
                        error_msg = "‚ö†Ô∏è **Proxy Error**: Could not refresh credentials.\n\nPlease ensure **Google Vertex AI Studio** is open in your browser and the Harvester script is active."
                        chunk = {
                            "id": "error-no-creds",
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": "vertex-ai-proxy",
                            "choices": [{"index": 0, "delta": {"content": error_msg}, "finish_reason": "stop"}]
                        }
                        yield f"data: {json.dumps(chunk)}\n\n"
                        yield "data: [DONE]\n\n"
                        return

        # 4. Send Request (with Retry Logic)
        max_retries = 1
        content_yielded = False # Track if any content chunk was yielded
        
        for attempt in range(max_retries + 1):
            
            creds = cred_manager.get_credentials()
            # Double check in case refresh failed but we have old creds
            if not creds:
                # Should be handled above, but just in case
                # If we are in a retry loop, this means refresh failed completely
                if attempt > 0:
                    break
                return # Should not happen if pre-flight check passed

            # 1. Prepare Request Data
            original_body = json.loads(creds['body'])
            
            # Extract System Prompt
            system_instruction = ""
            chat_history = []
            
            for msg in messages:
                if msg['role'] == 'system':
                    system_instruction += msg['content'] + "\n"
                elif msg['role'] == 'user':
                    parts = []
                    if isinstance(msg['content'], str):
                        parts.append({"text": msg['content']})
                    elif isinstance(msg['content'], list):
                        for part in msg['content']:
                            if part['type'] == 'text':
                                parts.append({"text": part['text']})
                            elif part['type'] == 'image_url':
                                image_url = part['image_url']['url']
                                if image_url.startswith('data:'):
                                    # Extract base64 data
                                    header, encoded = image_url.split(',', 1)
                                    mime_type = header.split(':')[1].split(';')[0]
                                    parts.append({
                                        "inlineData": {
                                            "mimeType": mime_type,
                                            "data": encoded
                                        }
                                    })
                    chat_history.append({"role": "user", "parts": parts})
                elif msg['role'] == 'assistant':
                    chat_history.append({"role": "model", "parts": [{"text": msg['content']}]})

            # 2. Construct New Body
            # We clone the harvested body structure to keep all the magic context/metadata
            new_variables = original_body.get('variables', {}).copy()
            
            # Update contents (Chat History)
            new_variables['contents'] = chat_history
            
            # Update System Instruction
            if system_instruction:
                new_variables['systemInstruction'] = {"parts": [{"text": system_instruction.strip()}]}

            # Disable Safety Filters
            new_variables['safetySettings'] = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_CIVIC_INTEGRITY", "threshold": "BLOCK_NONE"}
            ]
                
            # Update Model
            # Load model mapping from models.json
            model_map = {}
            try:
                with open(MODELS_CONFIG_FILE, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    model_map = config.get('alias_map', {})
            except Exception as e:
                print(f"‚ö†Ô∏è Error loading models.json: {e}")

            target_model = model_map.get(model, model)
            
            # Handle suffixes for thinking and resolution
            thinking_mode = None
            resolution_mode = None
            
            if target_model.endswith("-low"):
                target_model = target_model[:-4]
                thinking_mode = "low"
            elif target_model.endswith("-high"):
                target_model = target_model[:-5]
                thinking_mode = "high"
                
            if target_model.endswith("-1k"):
                resolution_mode = "1k"
                target_model = target_model[:-3]
            elif target_model.endswith("-2k"):
                resolution_mode = "2k"
                target_model = target_model[:-3]
            elif target_model.endswith("-4k"):
                resolution_mode = "4k"
                target_model = target_model[:-3]

            print(f"üîÑ Switching model to: {target_model} (requested: {model})")
            new_variables['model'] = target_model
            
            # Apply generation parameters from client
            if 'generationConfig' not in new_variables:
                new_variables['generationConfig'] = {}
            
            gen_config = new_variables['generationConfig']

            # Handle Thinking Config
            # Case 1: Explicit suffixes (-low, -high)
            if thinking_mode:
                gen_config['thinkingConfig'] = {"includeThoughts": True}
                if thinking_mode == 'low':
                     budget = 8192
                elif thinking_mode == 'high':
                     budget = 32768
                
                gen_config['thinkingConfig']['budget_token_count'] = budget
                gen_config['thinkingConfig']['thinkingBudget'] = budget
                print(f"‚ÑπÔ∏è Configured Thinking (Suffix): Mode={thinking_mode}, Budget={budget}")

            # Case 2: No suffix, but client provided max_tokens (treat as thinking budget for 3-pro)
            # Only applies if we haven't already set a thinking mode via suffix
            elif 'gemini-3-pro' in target_model and 'max_tokens' in kwargs and kwargs['max_tokens'] is not None:
                budget = int(kwargs['max_tokens'])
                # Only enable thinking if budget is reasonable for thinking (e.g. > 1024)
                # or if user explicitly wants it. Let's assume max_tokens on 3-pro implies thinking budget.
                gen_config['thinkingConfig'] = {
                    "includeThoughts": True,
                    "budget_token_count": budget,
                    "thinkingBudget": budget
                }
                print(f"‚ÑπÔ∏è Configured Thinking (Custom): Budget={budget}")
            
            # Handle Resolution (Image Generation)
            if resolution_mode:
                # Ensure responseModalities includes IMAGE
                if 'responseModalities' not in gen_config:
                    gen_config['responseModalities'] = ["TEXT", "IMAGE"]

                if 'imageConfig' not in gen_config:
                    gen_config['imageConfig'] = {}
                
                # Map resolution mode to Vertex AI imageSize strings
                # Based on logs: "imageSize": "4K"
                size_str_map = {
                    "1k": "1K", # Assumed based on 4K pattern
                    "2k": "2K", # Assumed based on 4K pattern
                    "4k": "4K"  # Confirmed from logs
                }
                
                if resolution_mode in size_str_map:
                    gen_config['imageConfig']['imageSize'] = size_str_map[resolution_mode]
                    
                    # Set other standard image generation parameters from logs
                    gen_config['imageConfig']['personGeneration'] = "ALLOW_ALL"
                    
                    if 'imageOutputOptions' not in gen_config['imageConfig']:
                        gen_config['imageConfig']['imageOutputOptions'] = {"mimeType": "image/png"}
                    
                    # Default to 1:1 if not specified, as resolution suffixes usually imply square
                    if 'aspectRatio' not in gen_config['imageConfig']:
                        gen_config['imageConfig']['aspectRatio'] = "1:1"
                    
                    print(f"‚ÑπÔ∏è Configured Image Generation: Size={gen_config['imageConfig'].get('imageSize')}, Ratio={gen_config['imageConfig'].get('aspectRatio')}")
            
            # CLEANUP: Remove model-specific configurations that might cause conflicts
            # If we switch models, old generation configs (like thinking) might be invalid.
            
            # Remove 'thinkingConfig' if present, unless the model is explicitly a thinking model
            if not thinking_mode:
                gen_config.pop('thinkingConfig', None)
                # Also check for snake_case just in case
                gen_config.pop('thinking_config', None)

            # Remove 'imageConfig' if NOT an image model (to be safe)
            if not resolution_mode:
                gen_config.pop('imageConfig', None)
                gen_config.pop('sampleImageSize', None)
                gen_config.pop('width', None)
                gen_config.pop('height', None)
            
            # Note: The exact field name might be 'thinkingConfig' or inside 'generationConfig'
            # Based on common Vertex AI payloads, let's check 'generationConfig'
            
            # Fix maxOutputTokens
            # Allow client to override max_tokens, otherwise default to harvested value or 65535
            # client_max_tokens = original_body.get('variables', {}).get('generationConfig', {}).get('maxOutputTokens')
            
            # Check if client provided max_tokens in the request body (OpenAI format)
            # Note: 'original_body' here is the harvested body. We need to check the incoming 'messages' or 'body' from the request.
            # But wait, 'stream_chat' doesn't receive the full request body, only 'messages' and 'model'.
            # Let's assume we want to restore the high limit.
            
            if isinstance(gen_config, dict):
                # Restore high limit or use a safe default
                # If the harvested token had a value, we keep it (unless we want to force it)
                # User requested to put it back to 65535
                if 'maxOutputTokens' in gen_config:
                    # Ensure it's at least 8192 if it was lowered, or just set to 65535 if missing/low
                    if gen_config['maxOutputTokens'] < 8192:
                            gen_config['maxOutputTokens'] = 65535
                else:
                    gen_config['maxOutputTokens'] = 65535
            
            if 'temperature' in kwargs and kwargs['temperature'] is not None:
                gen_config['temperature'] = float(kwargs['temperature'])
                print(f"‚ÑπÔ∏è Set temperature: {gen_config['temperature']}")
                
            if 'top_p' in kwargs and kwargs['top_p'] is not None:
                gen_config['topP'] = float(kwargs['top_p'])
                print(f"‚ÑπÔ∏è Set topP: {gen_config['topP']}")
                
            if 'top_k' in kwargs and kwargs['top_k'] is not None:
                gen_config['topK'] = int(kwargs['top_k'])
                print(f"‚ÑπÔ∏è Set topK: {gen_config['topK']}")
                
            if 'max_tokens' in kwargs and kwargs['max_tokens'] is not None:
                gen_config['maxOutputTokens'] = int(kwargs['max_tokens'])
                print(f"‚ÑπÔ∏è Set maxOutputTokens: {gen_config['maxOutputTokens']}")
                
            if 'stop' in kwargs and kwargs['stop'] is not None:
                gen_config['stopSequences'] = kwargs['stop'] if isinstance(kwargs['stop'], list) else [kwargs['stop']]
                print(f"‚ÑπÔ∏è Set stopSequences: {gen_config['stopSequences']}")

            # DEBUG: Print all generation config parameters for inspection
            if resolution_mode or thinking_mode:
                print("\nüîç --- DEBUG: Generation Config Parameters ---")
                print(json.dumps(gen_config, indent=2))
                print("---------------------------------------------\n")

            # Reassemble body
            new_body = {
                "querySignature": original_body.get('querySignature'), # Might need this?
                "operationName": original_body.get('operationName'),
                "variables": new_variables
            }
            
            # 3. Prepare Headers
            headers = creds['headers'].copy() # Copy to avoid mutating the cached credentials
            
            # Ensure critical headers are present and correct
            # Note: 'Cookie', 'User-Agent', 'Origin', 'Referer' should now be in creds['headers'] from the harvester
            
            headers['content-type'] = 'application/json'
            
            # Remove headers that httpx/network layer should handle or that might cause conflicts
            headers.pop('content-length', None)
            headers.pop('Content-Length', None)
            headers.pop('host', None)
            headers.pop('Host', None)
            headers.pop('connection', None)
            headers.pop('Connection', None)
            headers.pop('accept-encoding', None) # Let httpx handle decompression

            url = creds['url']
            
            print(f"üöÄ Sending request to Google Vertex AI (Attempt {attempt+1})...")
            try:
                # Use a try-finally block to ensure we handle cancellation if needed,
                # though async with handles cleanup automatically.
                async with self.client.stream('POST', url, headers=headers, json=new_body) as response:
                    print(f"üì° Response Status: {response.status_code}")
                    
                    if response.status_code != 200:
                        error_text = await response.aread()
                        print(f"‚ùå Google API Error: {response.status_code} - {error_text}")
                        
                        # Check for potential token expiration
                        if response.status_code in [400, 401, 403] and attempt < max_retries:
                            print(f"‚ö†Ô∏è Auth Error ({response.status_code}). Triggering UI refresh and waiting...")
                            
                            # Trigger UI Refresh
                            await request_token_refresh()
                            
                            # Wait for new credentials
                            refreshed = await cred_manager.wait_for_refresh(timeout=45)
                            if refreshed:
                                print("‚úÖ Credentials refreshed! Waiting 1s before retrying request...")
                                await asyncio.sleep(1) # Add 1 second delay
                                # Update headers/url with new credentials
                                new_creds = cred_manager.get_credentials()
                                headers = new_creds['headers'].copy()
                                headers['content-type'] = 'application/json'
                                headers.pop('content-length', None)
                                headers.pop('host', None)
                                url = new_creds['url']
                                continue # Retry loop
                            else:
                                print("‚ùå Refresh timed out.")
                        
                        # If we get here, it's a fatal error or retry failed
                        error_payload = {"error": {"message": f"Upstream Error: {response.status_code} - {error_text.decode()}", "type": "upstream_error"}}
                        yield f"data: {json.dumps(error_payload)}\n\n"
                        return

                    buffer = ""
                    chunk_count = 0
                    
                    # ... (Stream processing logic) ...
                    # We need to handle the stream inside the loop, but if it fails mid-stream due to auth (rare for 200 OK), we can't easily retry.
                    # However, we handled the "200 OK but error inside JSON" case before. We need to adapt that too.
                    
                    async for chunk in response.aiter_text():
                        chunk_count += 1
                        buffer += chunk
                        
                        while buffer:
                            # Skip whitespace
                            buffer = buffer.lstrip()
                            if not buffer:
                                break
                                
                            # Handle Google's JSON array format [obj, obj, ...]
                            if buffer.startswith('['):
                                buffer = buffer[1:]
                                continue
                            if buffer.startswith(','):
                                buffer = buffer[1:]
                                continue
                            if buffer.startswith(']'):
                                buffer = buffer[1:]
                                continue

                            try:
                                decoder = json.JSONDecoder()
                                obj, idx = decoder.raw_decode(buffer)
                                
                                for chunk_data in self.process_google_response(obj):
                                    yield chunk_data
                                    content_yielded = True # Mark that content was successfully yielded
                                
                                buffer = buffer[idx:]
                            except json.JSONDecodeError:
                                # Incomplete JSON, wait for more data
                                break
                            except AuthError as e:
                                raise e # Re-raise to be caught by the outer try-except
                            except Exception as e:
                                print(f"Error parsing stream chunk: {e}")
                                # Log the start of the buffer to debug unexpected characters
                                print(f"üêõ Debug Buffer (Start): {buffer[:100].strip()}")
                                
                                # Aggressive skip: Find the next JSON start character
                                next_json_start = -1
                                for char in ['[', '{']:
                                    try:
                                        idx = buffer.index(char)
                                        if next_json_start == -1 or idx < next_json_start:
                                            next_json_start = idx
                                    except ValueError:
                                        pass
                                
                                if next_json_start != -1:
                                    print(f"‚ö†Ô∏è Skipping {next_json_start} non-JSON characters.")
                                    buffer = buffer[next_json_start:]
                                else:
                                    # If no JSON start found, skip one char to avoid infinite loop
                                    buffer = buffer[1:]
                    
                    # If we successfully processed the stream, break the retry loop
                    break

            except AuthError as e:
                print(f"‚ö†Ô∏è Auth Error caught in stream: {e}")
                if attempt < max_retries:
                    print("üîÑ Triggering refresh and retrying...")
                    await request_token_refresh()
                    # Step 1: Wait for the new credentials to be harvested
                    refreshed = await cred_manager.wait_for_refresh(timeout=60)
                    if refreshed:
                        # Step 2: Wait for the frontend to confirm the UI is stable
                        ui_ready = await cred_manager.wait_for_refresh_complete(timeout=60)
                        if ui_ready:
                            print("‚úÖ Credentials and UI ready! Waiting 1s before retrying request...")
                            await asyncio.sleep(1) # Add 1 second delay
                            # Update headers/url with new credentials
                            new_creds = cred_manager.get_credentials()
                            headers = new_creds['headers'].copy()
                            headers['content-type'] = 'application/json'
                            headers.pop('content-length', None)
                            headers.pop('host', None)
                            url = new_creds['url']
                            continue # Retry the request
                        else:
                            print("‚ùå Frontend UI did not become ready in time.")
                    else:
                        print("‚ùå Credential refresh timed out.")

                error_payload = {"error": {"message": str(e), "type": "authentication_error"}}
                yield f"data: {json.dumps(error_payload)}\n\n"
                return

            except Exception as e:
                print(f"‚ùå Request failed: {e}")
                if attempt < max_retries:
                    continue
                error_payload = {"error": {"message": str(e), "type": "request_error"}}
                yield f"data: {json.dumps(error_payload)}\n\n"
                return # Stop generator on fatal error
        
        # If we exit the loop without returning, it means we successfully processed the stream.
        
        if not content_yielded:
            # If the stream finished but yielded no content, log a warning.
            # We rely on the client to handle the empty stream gracefully after receiving [DONE].
            print("‚ö†Ô∏è Proxy Warning: Google API returned an empty stream (200 OK but no content).")
            
        # Ensure the stream is properly terminated with [DONE]
        yield "data: [DONE]\n\n"

    def process_google_response(self, data: Dict[str, Any]) -> Generator[str, None, None]:
            """Converts Google's response format to OpenAI's SSE format, handling text and images."""
            try:
                if not data:
                    return
                
                # Debug: Log the raw data received from Google
                print(f"üîç Google Raw Chunk: {json.dumps(data, indent=2)[:500]}...")
    
                if 'error' in data:
                    print(f"‚ö†Ô∏è Google Stream Error: {data['error']}")
                    # This error is usually not fatal, just a part of the stream.
                    return
    
                if 'results' in data and data['results']:
                    for result in data['results']:
                        if not result: continue
    
                        if 'errors' in result:
                            for err in result['errors']:
                                msg = err.get('message', 'Unknown Error')
                                print(f"‚ö†Ô∏è Google API Error: {msg}")
                                if "Recaptcha" in msg or "token" in msg.lower() or "Authentication" in msg:
                                    raise AuthError(f"Authentication failed: {msg}")
                            continue
    
                        result_data = result.get('data')
                        if not result_data: continue
    
                        candidates = result_data.get('candidates')
                        if not candidates: continue
    
                        for candidate in candidates:
                            content = candidate.get('content') or {}
                            parts = content.get('parts') or []
    
                            for part in parts:
                                delta = {}
                                # --- Text Part ---
                                text = part.get('text', '')
                                if text:
                                    if part.get('thought', False):
                                        delta['reasoning_content'] = text
                                    else:
                                        delta['content'] = text
    
                                # --- Image Part (inline data) ---
                                inline_data = part.get('inlineData')
                                uri = part.get('uri') # Check for external URI
                                
                                if inline_data:
                                    mime_type = inline_data.get('mimeType')
                                    b64_data = inline_data.get('data')
                                    if mime_type and b64_data:
                                        # Format as a markdown image data URI
                                        image_md = f"![Generated Image](data:{mime_type};base64,{b64_data})"
                                        delta['content'] = image_md
                                elif uri:
                                    # Format as a markdown image URL
                                    image_md = f"![Generated Image]({uri})"
                                    delta['content'] = image_md
    
                                # --- Yield Chunk if we have content ---
                                if delta:
                                    chunk = {
                                        "id": f"chatcmpl-proxy-{uuid.uuid4()}",
                                        "object": "chat.completion.chunk",
                                        "created": int(time.time()),
                                        "model": "vertex-ai-proxy",
                                        "choices": [{"index": 0, "delta": delta, "finish_reason": None}]
                                    }
                                    yield f"data: {json.dumps(chunk)}\n\n"
    
                            # Check finish reason for the candidate
                            finish_reason = candidate.get('finishReason')
                            
                            # Only send finish chunk if it's a final stop reason AND not part of a thought process
                            # Note: We assume if 'thought' is present in any part, the finish reason might be premature.
                            is_thought_part = any(p.get('thought', False) for p in parts)
                            
                            if finish_reason in ['STOP', 'MAX_TOKENS'] and not is_thought_part:
                                finish_chunk = {
                                    "id": f"chatcmpl-proxy-finish-{uuid.uuid4()}",
                                    "object": "chat.completion.chunk",
                                    "created": int(time.time()),
                                    "model": "vertex-ai-proxy",
                                    "choices": [{"index": 0, "delta": {}, "finish_reason": finish_reason.lower()}]
                                }
                                yield f"data: {json.dumps(finish_chunk)}\n\n"
                            elif finish_reason in ['STOP', 'MAX_TOKENS'] and is_thought_part:
                                print("‚ö†Ô∏è Suppressing premature finishReason due to active thinking mode.")
            except AuthError:
                raise # Re-raise to be caught by the retry logic
            except Exception as e:
                print(f"Error processing response object: {e}")
                print(f"üêõ Debug Data causing error: {json.dumps(data, indent=2)}")

vertex_client = VertexAIClient()

# --- FastAPI App ---
app = FastAPI()

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"status": "running", "service": "Vertex AI Proxy"}

@app.get("/v1/models")
async def list_models(request: Request):
    # API Key Check
    if API_KEY:
        auth = request.headers.get("Authorization")
        if not auth or not auth.startswith("Bearer ") or auth[7:] != API_KEY:
            raise HTTPException(status_code=401, detail="Invalid API Key")

    # Return a list of common Vertex AI models
    # This helps clients know what's available
    current_time = int(time.time())
    models = []
    try:
        with open(MODELS_CONFIG_FILE, 'r', encoding='utf-8') as f:
            config = json.load(f)
            models = config.get('models', [])
    except Exception as e:
        print(f"‚ö†Ô∏è Error loading models.json: {e}")
        # Fallback
        models = ["gemini-1.5-pro", "gemini-1.5-flash"]

    data = {
        "object": "list",
        "data": [
            {"id": m, "object": "model", "created": current_time, "owned_by": "google"}
            for m in models
        ]
    }
    return data

@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    # API Key Check
    if API_KEY:
        auth = request.headers.get("Authorization")
        if not auth or not auth.startswith("Bearer ") or auth[7:] != API_KEY:
            raise HTTPException(status_code=401, detail="Invalid API Key")

    try:
        body = await request.json()
        messages = body.get('messages', [])
        model = body.get('model', 'gemini-1.5-pro')
        stream = body.get('stream', False) # Extract stream flag
        
        # Extract generation parameters
        temperature = body.get('temperature')
        top_p = body.get('top_p')
        top_k = body.get('top_k')
        max_tokens = body.get('max_tokens')
        stop = body.get('stop')
        
        if not messages:
            raise HTTPException(status_code=400, detail="No messages provided")

        if stream:
            return StreamingResponse(
                vertex_client.stream_chat(
                    messages,
                    model,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    max_tokens=max_tokens,
                    stop=stop
                ),
                media_type="text/event-stream"
            )
        else:
            # Non-streaming request
            response_data = await vertex_client.complete_chat(
                messages,
                model,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                max_tokens=max_tokens,
                stop=stop
            )
            return response_data

    except Exception as e:
        print(f"Error in endpoint: {e}")
        # FastAPI handles exceptions better, but for compatibility:
        raise HTTPException(status_code=500, detail={"error": str(e)})

# --- Admin Endpoints ---
@app.get("/admin")
async def admin_page(request: Request):
    # Simple HTML form to update cookies
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Vertex AI Proxy Admin</title>
        <style>
            body { font-family: sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; background: #1e1e1e; color: #e0e0e0; }
            textarea { width: 100%; height: 300px; background: #252526; color: #d4d4d4; border: 1px solid #3e3e42; padding: 10px; font-family: monospace; }
            button { background: #0e639c; color: white; border: none; padding: 10px 20px; cursor: pointer; margin-top: 10px; }
            button:hover { background: #1177bb; }
            .card { background: #252526; padding: 20px; border-radius: 8px; margin-bottom: 20px; }
            h2 { margin-top: 0; }
        </style>
    </head>
    <body>
        <h1>üõ†Ô∏è Admin Dashboard</h1>
        
        <div class="card">
            <h2>üç™ Update Cloud Cookies</h2>
            <p>Paste your exported Google Cookies (JSON) here to hot-reload the Cloud Harvester.</p>
            <form action="/admin/update_cookies" method="post">
                <textarea name="cookies" placeholder='[{"domain": ".google.com", ...}]'></textarea>
                <br>
                <input type="password" name="api_key" placeholder="API Key (if enabled)" style="padding: 8px; width: 200px; margin-top: 10px; background: #3e3e42; color: white; border: 1px solid #555;">
                <button type="submit">Update Cookies</button>
            </form>
        </div>
    </body>
    </html>
    """
    return StreamingResponse(iter([html_content]), media_type="text/html")

@app.post("/admin/update_cookies")
async def update_cookies(request: Request):
    form = await request.form()
    cookies = form.get("cookies")
    api_key = form.get("api_key")
    
    # Security Check
    if API_KEY and api_key != API_KEY:
        return StreamingResponse(iter(["<h1>‚ùå Invalid API Key</h1>"]), media_type="text/html", status_code=401)
        
    if not cookies:
        return StreamingResponse(iter(["<h1>‚ùå No cookies provided</h1>"]), media_type="text/html", status_code=400)
    
    # Validate JSON
    try:
        json.loads(cookies)
    except json.JSONDecodeError:
        return StreamingResponse(iter(["<h1>‚ùå Invalid JSON format</h1>"]), media_type="text/html", status_code=400)
        
    # Update Harvester
    if 'harvester' in globals() and harvester:
        await harvester.update_cookies(cookies)
        return StreamingResponse(iter(["<h1>‚úÖ Cookies Updated! Harvester restarting...</h1><a href='/admin'>Back</a>"]), media_type="text/html")
    else:
        return StreamingResponse(iter(["<h1>‚ö†Ô∏è Cloud Harvester is not running. (Did you set GOOGLE_COOKIES env var?)</h1>"]), media_type="text/html")

# --- WebSocket Server (For Harvester) ---
# Store connected harvester clients
harvester_clients: set[WebSocket] = set()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    print("üîå WebSocket client connected")
    harvester_clients.add(websocket)
    try:
        while True:
            message = await websocket.receive_text()
            try:
                data = json.loads(message)
                msg_type = data.get("type")
                if msg_type == "credentials_harvested":
                    cred_manager.update(data.get("data"))
                elif msg_type == "token_refreshed":
                    cred_manager.update_token(data.get("token"))
                elif msg_type == "refresh_complete":
                    print("‚úÖ Frontend confirms refresh is complete.")
                    cred_manager.refresh_complete_event.set()
                elif msg_type == "identify":
                    print(f"üëã Client identified: {data.get('client')}")
            except Exception as e:
                print(f"WS Error: {e}")
    except WebSocketDisconnect:
        print("üîå WebSocket client disconnected")
        harvester_clients.remove(websocket)
    except Exception as e:
        print(f"WS Handler Error: {e}")
        if websocket in harvester_clients:
            harvester_clients.remove(websocket)

async def request_token_refresh():
    print("üîÑ Requesting token refresh...")
    
    # 1. Trigger Cloud Harvester (if running)
    if 'harvester' in globals() and harvester and harvester.is_running:
        print("‚òÅÔ∏è Triggering Cloud Harvester...")
        # We don't await this because perform_harvest might take time,
        # and we want to trigger WS clients too.
        # But wait, perform_harvest is async. We should probably fire and forget or await?
        # Since we are inside a request handler (stream_chat), awaiting might block.
        # But we need the result.
        # Actually, CloudHarvester loop runs periodically. We can force an immediate run.
        # Let's add a method to CloudHarvester to force harvest.
        asyncio.create_task(harvester.perform_harvest())
        return # If we have a cloud harvester, we might not need WS clients, or maybe both?
               # Let's try both just in case.

    # 2. Trigger WebSocket Clients (Local Browser)
    if not harvester_clients:
        print("‚ö†Ô∏è No harvester clients connected!")
        return
    
    print("üîå Requesting refresh from WebSocket clients...")
    message = json.dumps({"type": "refresh_token"})
    # Broadcast to all connected harvesters
    for ws in list(harvester_clients):
        try:
            await ws.send_text(message)
        except Exception as e:
            print(f"Failed to send refresh request: {e}")
            # WebSocketDisconnect is handled in the endpoint loop usually,
            # but if send fails we might want to remove it.
            if ws in harvester_clients:
                harvester_clients.remove(ws)

async def keep_alive_loop():
    """Background task to refresh credentials periodically (every 45 mins)."""
    print("‚è∞ Keep-Alive Task Started")
    while True:
        try:
            # Wait for 45 minutes (2700 seconds)
            # We check every minute to see if we need to exit or if we should trigger early
            for _ in range(45):
                await asyncio.sleep(60)
            
            print("‚è∞ Keep-Alive: Triggering scheduled refresh...")
            await request_token_refresh()
            
        except asyncio.CancelledError:
            break
        except Exception as e:
            print(f"‚ö†Ô∏è Keep-Alive Error: {e}")
            await asyncio.sleep(60)

async def main():
    config = uvicorn.Config(app, host="0.0.0.0", port=PORT, log_level="info")
    server = uvicorn.Server(config)

    print(f"\nüöÄ Headful Proxy Started")
    print(f"   - Address: http://0.0.0.0:{PORT}")
    print(f"   - WebSocket: ws://0.0.0.0:{PORT}/ws")
    if API_KEY:
        print(f"   - Security: API Key enabled")
    
    # --- Cloud Harvester Integration ---
    # Check if we should run the cloud harvester (requires GOOGLE_COOKIES)
    # Make harvester global so admin endpoint can access it
    global harvester
    harvester = None
    
    # Check if we should run the cloud harvester
    # 1. Explicitly enabled via ENABLE_AUTO_HARVEST
    # 2. Implicitly enabled if GOOGLE_COOKIES is set
    enable_cloud = os.environ.get("ENABLE_AUTO_HARVEST", "false").lower() == "true"
    if os.environ.get("GOOGLE_COOKIES"):
        enable_cloud = True

    if enable_cloud:
        try:
            from cloud_harvester import CloudHarvester
            harvester = CloudHarvester(cred_manager)
            # Run harvester in background
            asyncio.create_task(harvester.start())
            print("‚òÅÔ∏è Cloud Harvester initialized (Experimental).")
        except ImportError:
            print("‚ö†Ô∏è Cloud Harvester dependencies (playwright) not found.")
    else:
        print("   üëâ Please ensure the 'Harvester' userscript is running in your browser.")

    # Start Keep-Alive Loop to proactively refresh tokens
    asyncio.create_task(keep_alive_loop())

    await server.serve()

if __name__ == "__main__":
    if HEADLESS:
        print("üñ•Ô∏è Running in HEADLESS mode")
        asyncio.run(main())
    else:
        try:
            import gui
            def server_runner():
                asyncio.run(main())
            gui.run(server_runner, stats_manager)
        except ImportError:
            print("‚ö†Ô∏è GUI dependencies not found or failed. Falling back to headless mode.")
            asyncio.run(main())